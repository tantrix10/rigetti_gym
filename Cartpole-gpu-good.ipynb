{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import gym\n",
    "#import pennylane\n",
    "from pyquil import Program\n",
    "from pyquil.api import get_qc, WavefunctionSimulator, local_qvm\n",
    "from pyquil.gates import *\n",
    "import numpy as np\n",
    "import math\n",
    "import random as pr\n",
    "\n",
    "#import os, inspect, sys\n",
    "#import matplotlib.pyplot as plt\n",
    "#import sys\n",
    "#from operator import xor\n",
    "#sys.path.insert(0, 'tests/')\n",
    "#sys.path.insert(0, 'auxiliary_functions/')\n",
    "\n",
    "#from auxiliary_functions.auxiliary_rb import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qc_name = '4q-qvm'\n",
    "with local_qvm():\n",
    "    qc = get_qc(qc_name)\n",
    "\n",
    "qubits = qc.qubits()\n",
    "\n",
    "rand = [np.random.choice(2) for i in range(6)]\n",
    "#n_samples = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def circ(state, policy,n_samples):\n",
    "    p = Program()\n",
    "    p += [H(i) for i in qubits]\n",
    "    p += [RZ(state[i], i) for i in qubits]\n",
    "    for i in range(len(qubits)):\n",
    "        for j in range(i+1):\n",
    "            if i != j:\n",
    "                p += CZ(i,j)\n",
    "    p += [RZ(state[0]*state[1], i),RZ(state[1]*state[0], i),RZ(state[2]*state[3], i),RZ(state[3]*state[1], i)]\n",
    "    p += [RZ(policy[i], i) for i in qubits]\n",
    "    count = 0\n",
    "    for i in range(len(qubits)):\n",
    "        for j in range(i+1):\n",
    "            if i != j:\n",
    "                if rand[count] == 1:\n",
    "                    p += CZ(i,j)\n",
    "                count+=1\n",
    "    n = len(qubits)\n",
    "    ro = p.declare('ro', 'BIT',n)\n",
    "    p += [MEASURE(qubits[i], ro[i]) for i in qubits]\n",
    "    with local_qvm():\n",
    "        p.wrap_in_numshots_loop(n_samples)\n",
    "        quil_ex = qc.compile(p)\n",
    "        samples = qc.run(quil_ex) # Use 'run' not 'run_and_measure' here\n",
    "    if n_samples == 1:\n",
    "        votes = [1 if np.count_nonzero(samples[:,i]) == 1 else 0 for i in range(4)]\n",
    "        #print(votes)\n",
    "        vote1 = xor(votes[0],votes[1])\n",
    "        vote2 = xor(votes[2],votes[3])\n",
    "        #print(samples, votes, vote1, vote2)\n",
    "        return xor(vote1, vote2)\n",
    "    else:\n",
    "        actions = []\n",
    "        for j in range(n_samples):\n",
    "            votes = [1 if np.count_nonzero(samples[j,i]) == 1 else 0 for i in range(4)]\n",
    "            #print(votes)\n",
    "            vote1 = xor(votes[0],votes[1])\n",
    "            vote2 = xor(votes[2],votes[3])\n",
    "            #print(samples, votes, vote1, vote2)\n",
    "            actions.append( xor(vote1, vote2))\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nif __name__ == '__main__':\\n\\n    cp = CartPole()\\n\\n    global ccount\\n    ccount = 1\\n    print(cp.single_episode(policy))\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#credit to Jeremy Stober for the original simulation library github.com/stober, code adapted from here.\n",
    "\n",
    "class CartPole(object):\n",
    "\n",
    "    def __init__(self, x = 0.0, xdot = 0.0, theta = 0.0, thetadot = 0.0):\n",
    "        self.x = x\n",
    "        self.xdot = xdot\n",
    "        self.theta = theta\n",
    "        self.thetadot = thetadot\n",
    "\n",
    "        # some constants\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5\t\t  # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02\t\t  # seconds between state updates\n",
    "        self.fourthirds = 1.3333333333333\n",
    "\n",
    "    def failure(self):\n",
    "        twelve_degrees = 0.2094384\n",
    "        if (not -2.4 < self.x < 2.4) or (not -twelve_degrees < self.theta < twelve_degrees):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def reset(self):\n",
    "        self.x,self.xdot,self.theta,self.thetadot = (0.0,0.0,0.0,0.0)\n",
    "\n",
    "\n",
    "    def single_episode(self, policy):\n",
    "        self.reset()\n",
    "        #if policy is None: policy = self.random_policy\n",
    "\n",
    "        trace = []\n",
    "        next_action = circ([self.x,self.xdot,self.theta,self.thetadot], policy, 1)\n",
    "        while not self.failure():\n",
    "            #state = self.move(next_action)\n",
    "            #next_action = circ(state, policy)\n",
    "            #print('nudge',next_action,state)\n",
    "            #trace.append([1])\n",
    "            pstate, paction, reward, state = self.move(next_action)\n",
    "            next_action                    =  circ(state, policy, 1)\n",
    "            trace.append([pstate, paction])\n",
    "        #gamma = 0.9\n",
    "        #tot_rew = len(trace)\n",
    "        return trace #(1-gamma**tot_rew)/(1-gamma)\n",
    "\n",
    "\n",
    "    def reward(self):\n",
    "        if self.failure():\n",
    "            return 0.0\n",
    "        else:\n",
    "            return 1.0\n",
    "\n",
    "    def move(self, action, boxed=False): # binary L/R action\n",
    "        force = 0.0\n",
    "        if action > 0:\n",
    "            force = self.force_mag\n",
    "        else:\n",
    "            force = -self.force_mag\n",
    "\n",
    "        costheta = math.cos(self.theta)\n",
    "        sintheta = math.sin(self.theta)\n",
    "\n",
    "        tmp = (force + self.polemass_length * (self.thetadot ** 2) * sintheta) / self.total_mass;\n",
    "        thetaacc = (self.gravity * sintheta - costheta * tmp) / (self.length * (self.fourthirds - self.masspole * costheta ** 2 / self.total_mass))\n",
    "        xacc = tmp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        (px,pxdot,ptheta,pthetadot) = (self.x, self.xdot, self.theta, self.thetadot)\n",
    "        #pstate = self.state()\n",
    "\n",
    "        self.x += self.tau * self.xdot\n",
    "        self.xdot += self.tau * xacc\n",
    "        self.theta += self.tau * self.thetadot\n",
    "        self.thetadot += self.tau * thetaacc\n",
    "        return [px,pxdot,ptheta,pthetadot],action,self.reward(),[self.x,self.xdot, self.theta, self.thetadot]\n",
    "\n",
    "    \n",
    "def grad_est(state, action, policy):\n",
    "    n_samples = 20 #this will need to be much higher\n",
    "    delta = 1e-4\n",
    "    sample = circ(state, policy, n_samples)\n",
    "    #[print(sample)]\n",
    "    prob_est = sample.count(action)/n_samples\n",
    "    if prob_est == 0:\n",
    "        prob_est = 1e-4\n",
    "    #print(prob_est)\n",
    "    prob_del = []\n",
    "    for i in range(len(policy)):\n",
    "        temp_p = policy[:]\n",
    "        temp_p[i] += delta\n",
    "        sample = circ(state, temp_p, n_samples)\n",
    "        prob_est = sample.count(action)/n_samples\n",
    "        if prob_est == 0:\n",
    "            prob_est = 1e-4\n",
    "        prob_del.append(prob_est)\n",
    "        #print(prob_del)\n",
    "    return [(np.log(prob_del[i])-np.log(prob_est))/delta if prob_del[i] ==0.0 or prob_est == 0.0 else ((prob_del[i])-(prob_est))/delta for i in range(len(policy))]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "policy = [np.random.random() for i in range(4)]\n",
    "cp = CartPole()\n",
    "learning_rate = 0.02\n",
    "gamma = 0.9\n",
    "ks = []\n",
    "gradss = []\n",
    "policies =[]\n",
    "for i in range(num_episodes):\n",
    "    episode = cp.single_episode(policy)\n",
    "    k = len(episode)\n",
    "    print(k)\n",
    "    ks.append(k)\n",
    "    grads = [grad_est(episode[i][0], episode[i][1], policy) for i in range(k)]\n",
    "    gradss.append(grads)\n",
    "    #print('grads :',grads)\n",
    "    for j in range(k):\n",
    "        for l in range(4):\n",
    "            policy[l] += learning_rate*grads[j][l]*(1-gamma**(i+1))/(1-gamma)\n",
    "            policies.append(policy)\n",
    "            #print('polices :', policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
